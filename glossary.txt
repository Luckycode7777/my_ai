np.random.seed(42)
Если seed задан вручную, например, random.seed(42), алгоритм каждый раз начинает с одной и той же «страницы».
В результате всегда получается одинаковая последовательность «случайных» чисел.

===
Эмбеддинги — это не просто векторы, а логарифмы условных вероятностей в латентном пространстве.
Байесовский взгляд помогает понять, почему модель "изучает" контексты.
Слишком малая размерность снижает выразительность.
Слишком большая — приводит к переобучению и лишним параметрам.
Размерность embedding стоит подбирать эмпирически, в зависимости от данных.

===
Позиционные вектора необходимы для трансформеров, чтобы понять порядок слов.
Синусоидальные:
Необучаемые, интерпретируемые
Хорошо экстраполируют на длинные последовательности
Обучаемые:
Гибкие, адаптируются к задаче
Могут переобучаться на фиксированные длины
Оба подхода работают, но в разных задачах может быть предпочтение к одному или другому.

===
Токен
Текущий токен
Веса
01:05:00
  