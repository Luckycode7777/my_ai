np.random.seed(42)
Если seed задан вручную, например, random.seed(42), алгоритм каждый раз начинает с одной и той же «страницы».
В результате всегда получается одинаковая последовательность «случайных» чисел.

===
Эмбеддинги — это не просто векторы, а логарифмы условных вероятностей в латентном пространстве.
Байесовский взгляд помогает понять, почему модель "изучает" контексты.
Слишком малая размерность снижает выразительность.
Слишком большая — приводит к переобучению и лишним параметрам.
Размерность embedding стоит подбирать эмпирически, в зависимости от данных.

===
Позиционные вектора необходимы для трансформеров, чтобы понять порядок слов.
Синусоидальные:
Необучаемые, интерпретируемые
Хорошо экстраполируют на длинные последовательности
Обучаемые:
Гибкие, адаптируются к задаче
Могут переобучаться на фиксированные длины
Оба подхода работают, но в разных задачах может быть предпочтение к одному или другому.

===
Токен
Текущий токен
Веса
01:05:00

===
Формула y = f(Σ(w_i * x_i) + b) описывает работу одного нейрона в нейронной сети.

В ней:
Σ - сумма
x_i — входные сигналы;
w_i — веса соответствующих входов;
b — смещение (bias), позволяющее настраивать порог активации;
f — активационная функция;
y — выходной сигнал.

__________________________________________________________________________
1. Σ (прописная греческая буква "сигма") в математике обозначает суммирование (сложение). Читается как "сумма".
В контексте формулы нейрона выражение Σ(w_i * x_i)
означает: "Взять все пары весов и входов, перемножить их, а затем сложить получившиеся произведения".

Если расписать это подробно, то получится:
Σ(w_i * x_i) = w₁ * x₁ + w₂ * x₂ + w₃ * x₃ + ... + w_n * x_n

Простыми словами: это этап, на котором нейрон собирает всю пришедшую к нему информацию,
но пока не принимает решения (не активируется), а только взвешивает ее (умножает важность каждого сигнала на сам сигнал).

2. b (bias — смещение)
b (от англ. bias — предубеждение, сдвиг, смещение) — это специальная добавка,
которая прибавляется к взвешенной сумме входов до того, как сработает функция активации.

Простая аналогия:
Представьте, что нейрон — это строгий учитель, который ставит "зачет" (y = 1), если сумма баллов за
домашние работы (Σ(w_i * x_i)) больше или равна проходному баллу.

Без смещения: Проходной балл жестко зафиксирован на нуле. Если ученик набрал -1 балл (был штраф),
 он автоматически не сдал. Если набрал 2 — сдал.
Со смещением: Учитель может сказать: "Я даю вам всем фору (bias) в 5 баллов просто так, а теперь посмотрим, что вы накопили".
 В этом случае итоговая сумма перед оценкой будет: (Накопленные баллы) + 5.

Зачем это нужно в нейросетях?
Смещение (b) позволяет сдвинуть график функции активации влево или вправо.
Это дает нейрону возможность активироваться (выдавать сигнал), даже если сумма входных сигналов очень маленькая или отрицательная.
И наоборот, можно сделать нейрон "ленивым", чтобы он активировался только при очень сильном сигнале.

Если b положительное — нейрону легче сработать (порог как бы понижается).
Если b отрицательное — нейрону труднее сработать (порог повышается).
Без смещения нейрон всегда проходит через точку ноль, что сильно ограничивает его гибкость в обучении.
Bias добавляет нейрону "личное мнение", независимое от входных сигналов.
__________________________________________________________________________